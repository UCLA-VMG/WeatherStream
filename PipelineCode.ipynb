{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeff29b4",
   "metadata": {
    "id": "aeff29b4"
   },
   "outputs": [],
   "source": [
    "# Required imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision\n",
    "import gc\n",
    "from torch.nn import init\n",
    "\n",
    "import functools\n",
    "from PIL import Image\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from natsort import natsorted\n",
    "from glob import glob, escape\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim \n",
    "from scipy.signal import medfilt\n",
    "from scipy import ndimage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df55608",
   "metadata": {
    "id": "2df55608"
   },
   "outputs": [],
   "source": [
    "# Main network blocks 1\n",
    "# Code taken from GT-RAIN: https://github.com/UCLA-VMG/GT-RAIN\n",
    "\n",
    "# Code modified from: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\n",
    "\n",
    "# Basic Blocks\n",
    "class Identity(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "def get_norm_layer(norm_type='instance'):\n",
    "    \"\"\"Return a normalization layer\n",
    "    Parameters:\n",
    "            norm_type (str) -- the name of the normalization layer: batch | instance | none\n",
    "    For BatchNorm, we use learnable affine parameters and track running statistics (mean/stddev).\n",
    "    For InstanceNorm, we do not use learnable affine parameters. We do not track running statistics.\n",
    "    \"\"\"\n",
    "    if norm_type == 'batch':\n",
    "        norm_layer = functools.partial(nn.BatchNorm2d, affine=True, track_running_stats=True)\n",
    "    elif norm_type == 'instance':\n",
    "        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)\n",
    "    elif norm_type == 'none':\n",
    "        def norm_layer(x): return Identity()\n",
    "    else:\n",
    "        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n",
    "    return norm_layer\n",
    "\n",
    "class Conv2d(torch.nn.Module):\n",
    "    '''\n",
    "    2D convolution class\n",
    "    Args:\n",
    "        in_channels : int\n",
    "            number of input channels\n",
    "        out_channels : int\n",
    "            number of output channels\n",
    "        kernel_size : int\n",
    "            size of kernel\n",
    "        stride : int\n",
    "            stride of convolution\n",
    "        activation_func : func\n",
    "            activation function after convolution\n",
    "        norm_layer : functools.partial\n",
    "            normalization layer\n",
    "        use_bias : bool\n",
    "            if set, then use bias\n",
    "        padding_type : str\n",
    "            the name of padding layer: reflect | replicate | zero\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            activation_func=torch.nn.LeakyReLU(negative_slope=0.10, inplace=True),\n",
    "            norm_layer=nn.BatchNorm2d,\n",
    "            use_bias=False,\n",
    "            padding_type='reflect'):\n",
    "        super(Conv2d, self).__init__()\n",
    "        \n",
    "        self.activation_func = activation_func\n",
    "        conv_block = []\n",
    "        p = 0\n",
    "        if padding_type == 'reflect':\n",
    "            conv_block += [nn.ReflectionPad2d(kernel_size // 2)]\n",
    "        elif padding_type == 'replicate':\n",
    "            conv_block += [nn.ReplicationPad2d(kernel_size // 2)]\n",
    "        elif padding_type == 'zero':\n",
    "            p = kernel_size // 2\n",
    "        else:\n",
    "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
    "\n",
    "        conv_block += [\n",
    "                nn.Conv2d(\n",
    "                        in_channels, \n",
    "                        out_channels, \n",
    "                        stride=stride,\n",
    "                        kernel_size=kernel_size, \n",
    "                        padding=p, \n",
    "                        bias=use_bias), \n",
    "                norm_layer(out_channels)]\n",
    "\n",
    "        self.conv = nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv = self.conv(x)\n",
    "\n",
    "        if self.activation_func is not None:\n",
    "            return self.activation_func(conv)\n",
    "        else:\n",
    "            return conv\n",
    "\n",
    "class DeformableConv2d(nn.Module):\n",
    "    '''\n",
    "    2D deformable convolution class\n",
    "    Args:\n",
    "        in_channels : int\n",
    "            number of input channels\n",
    "        out_channels : int\n",
    "            number of output channels\n",
    "        kernel_size : int\n",
    "            size of kernel\n",
    "        stride : int\n",
    "            stride of convolution\n",
    "        padding : int\n",
    "            padding\n",
    "        use_bias : bool\n",
    "            if set, then use bias\n",
    "    '''\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            bias=False):\n",
    "\n",
    "        super(DeformableConv2d, self).__init__()\n",
    "        \n",
    "        self.stride = stride if type(stride) == tuple else (stride, stride)\n",
    "        self.padding = padding\n",
    "        \n",
    "        self.offset_conv = nn.Conv2d(\n",
    "                in_channels, \n",
    "                2 * kernel_size * kernel_size,\n",
    "                kernel_size=kernel_size, \n",
    "                stride=stride,\n",
    "                padding=self.padding, \n",
    "                bias=True)\n",
    "\n",
    "        nn.init.constant_(self.offset_conv.weight, 0.)\n",
    "        nn.init.constant_(self.offset_conv.bias, 0.)\n",
    "        \n",
    "        self.modulator_conv = nn.Conv2d(\n",
    "                in_channels, \n",
    "                1 * kernel_size * kernel_size,\n",
    "                kernel_size=kernel_size, \n",
    "                stride=stride,\n",
    "                padding=self.padding, \n",
    "                bias=True)\n",
    "\n",
    "        nn.init.constant_(self.modulator_conv.weight, 0.)\n",
    "        nn.init.constant_(self.modulator_conv.bias, 0.)\n",
    "        \n",
    "        self.regular_conv = nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                padding=self.padding,\n",
    "                bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        offset = self.offset_conv(x)\n",
    "        modulator = 2. * torch.sigmoid(self.modulator_conv(x))\n",
    "        \n",
    "        x = torchvision.ops.deform_conv2d(\n",
    "                input=x, \n",
    "                offset=offset, \n",
    "                weight=self.regular_conv.weight, \n",
    "                bias=self.regular_conv.bias, \n",
    "                padding=self.padding,\n",
    "                mask=modulator,\n",
    "                stride=self.stride)\n",
    "        return x\n",
    "\n",
    "class UpConv2d(torch.nn.Module):\n",
    "    '''\n",
    "    Up-convolution (upsample + convolution) block class\n",
    "    Args:\n",
    "        in_channels : int\n",
    "            number of input channels\n",
    "        out_channels : int\n",
    "            number of output channels\n",
    "        kernel_size : int\n",
    "            size of kernel (k x k)\n",
    "        activation_func : func\n",
    "            activation function after convolution\n",
    "        norm_layer : functools.partial\n",
    "            normalization layer\n",
    "        use_bias : bool\n",
    "            if set, then use bias\n",
    "        padding_type : str\n",
    "            the name of padding layer: reflect | replicate | zero\n",
    "        interpolate_mode : str\n",
    "            the mode for interpolation: bilinear | nearest\n",
    "    '''\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            activation_func=torch.nn.LeakyReLU(negative_slope=0.10, inplace=True),\n",
    "            norm_layer=nn.BatchNorm2d,\n",
    "            use_bias=False,\n",
    "            padding_type='reflect',\n",
    "            interpolate_mode='bilinear'):\n",
    "        \n",
    "        super(UpConv2d, self).__init__()\n",
    "        self.interpolate_mode = interpolate_mode\n",
    "\n",
    "        self.conv = Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=1,\n",
    "                activation_func=activation_func,\n",
    "                norm_layer=norm_layer,\n",
    "                use_bias=use_bias,\n",
    "                padding_type=padding_type)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n_height, n_width = x.shape[2:4]\n",
    "        shape = (int(2 * n_height), int(2 * n_width))\n",
    "        upsample = torch.nn.functional.interpolate(\n",
    "                x, size=shape, mode=self.interpolate_mode, align_corners=True)\n",
    "        conv = self.conv(upsample)\n",
    "        return conv\n",
    "\n",
    "class DeformableResnetBlock(nn.Module):\n",
    "    \"\"\"Define a Resnet block with deformable convolutions\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, dim, padding_type, \n",
    "            norm_layer, use_dropout, \n",
    "            use_bias, activation_func):\n",
    "        \"\"\"Initialize the deformable Resnet block\n",
    "        A defromable resnet block is a conv block with skip connections\n",
    "        \"\"\"\n",
    "        super(DeformableResnetBlock, self).__init__()\n",
    "        self.conv_block = self.build_conv_block(\n",
    "                dim, padding_type, \n",
    "                norm_layer, use_dropout, \n",
    "                use_bias, activation_func)\n",
    "\n",
    "    def build_conv_block(\n",
    "            self, dim, padding_type, \n",
    "            norm_layer, use_dropout, \n",
    "            use_bias, activation_func):\n",
    "        \"\"\"Construct a convolutional block.\n",
    "        Parameters:\n",
    "                dim (int) -- the number of channels in the conv layer.\n",
    "                padding_type (str) -- the name of padding layer: reflect | replicate | zero\n",
    "                norm_layer -- normalization layer\n",
    "                use_dropout (bool) -- if use dropout layers.\n",
    "                use_bias (bool) -- if the conv layer uses bias or not\n",
    "                activation_func (func) -- activation type\n",
    "        Returns a conv block (with a conv layer, a normalization layer, and a non-linearity layer)\n",
    "        \"\"\"\n",
    "        conv_block = []\n",
    "\n",
    "        p = 0\n",
    "        if padding_type == 'reflect':\n",
    "            conv_block += [nn.ReflectionPad2d(1)]\n",
    "        elif padding_type == 'replicate':\n",
    "            conv_block += [nn.ReplicationPad2d(1)]\n",
    "        elif padding_type == 'zero':\n",
    "            p = 1\n",
    "        else:\n",
    "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
    "\n",
    "        conv_block += [\n",
    "                DeformableConv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), \n",
    "                norm_layer(dim), \n",
    "                activation_func]\n",
    "        if use_dropout:\n",
    "            conv_block += [nn.Dropout(0.5)]\n",
    "\n",
    "        p = 0\n",
    "        if padding_type == 'reflect':\n",
    "            conv_block += [nn.ReflectionPad2d(1)]\n",
    "        elif padding_type == 'replicate':\n",
    "            conv_block += [nn.ReplicationPad2d(1)]\n",
    "        elif padding_type == 'zero':\n",
    "            p = 1\n",
    "        else:\n",
    "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
    "        conv_block += [DeformableConv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim)]\n",
    "\n",
    "        return nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function (with skip connections)\"\"\"\n",
    "        out = x + self.conv_block(x)        # add skip connections\n",
    "        return out\n",
    "\n",
    "class DecoderBlock(torch.nn.Module):\n",
    "    '''\n",
    "    Decoder block with skip connections\n",
    "    Args:\n",
    "        in_channels : int\n",
    "            number of input channels\n",
    "        skip_channels : int\n",
    "            number of skip connection channels\n",
    "        out_channels : int\n",
    "            number of output channels\n",
    "        activation_func : func\n",
    "            activation function after convolution\n",
    "        norm_layer : functools.partial\n",
    "            normalization layer\n",
    "        use_bias : bool\n",
    "            if set, then use bias\n",
    "        padding_type : str\n",
    "            the name of padding layer: reflect | replicate | zero\n",
    "        upsample_mode : str\n",
    "            the mode for interpolation: transpose | bilinear | nearest\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            skip_channels,\n",
    "            out_channels,\n",
    "            activation_func=torch.nn.LeakyReLU(negative_slope=0.10, inplace=True),\n",
    "            norm_layer=nn.BatchNorm2d,\n",
    "            use_bias=False,\n",
    "            padding_type='reflect',\n",
    "            upsample_mode='transpose'):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.skip_channels = skip_channels\n",
    "        self.upsample_mode = upsample_mode\n",
    "        \n",
    "        # Upsampling\n",
    "        if upsample_mode == 'transpose':\n",
    "            self.deconv = nn.Sequential(\n",
    "                    nn.ConvTranspose2d(\n",
    "                            in_channels, out_channels,\n",
    "                            kernel_size=3, stride=2,\n",
    "                            padding=1, output_padding=1,\n",
    "                            bias=use_bias),\n",
    "                    norm_layer(out_channels),\n",
    "                    activation_func)\n",
    "        else:\n",
    "            self.deconv = UpConv2d(\n",
    "                    in_channels, out_channels,\n",
    "                    use_bias=use_bias,\n",
    "                    activation_func=activation_func,\n",
    "                    norm_layer=norm_layer,\n",
    "                    padding_type=padding_type,\n",
    "                    interpolate_mode=upsample_mode)\n",
    "\n",
    "        concat_channels = skip_channels + out_channels\n",
    "        \n",
    "        self.conv = Conv2d(\n",
    "                concat_channels,\n",
    "                out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                activation_func=activation_func,\n",
    "                padding_type=padding_type,\n",
    "                norm_layer=norm_layer,\n",
    "                use_bias=use_bias)\n",
    "\n",
    "    def forward(self, x, skip=None):\n",
    "        deconv = self.deconv(x)\n",
    "\n",
    "        if self.skip_channels > 0:\n",
    "            concat = torch.cat([deconv, skip], dim=1)\n",
    "        else:\n",
    "            concat = deconv\n",
    "\n",
    "        return self.conv(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c803b7a",
   "metadata": {
    "id": "1c803b7a"
   },
   "outputs": [],
   "source": [
    "# Main network blocks 2\n",
    "# Code taken from GT-RAIN: https://github.com/UCLA-VMG/GT-RAIN\n",
    "\n",
    "def init_weights(net, init_type='normal', init_gain=0.02):\n",
    "    \"\"\"\n",
    "    Initialize network weights.\n",
    "    Parameters:\n",
    "            net (network) -- network to be initialized\n",
    "            init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n",
    "            init_gain (float) -- scaling factor for normal, xavier and orthogonal.\n",
    "    \"\"\"\n",
    "    def init_func(m):    # define the initialization function\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "            if init_type == 'normal':\n",
    "                init.normal_(m.weight.data, 0.0, init_gain)\n",
    "            elif init_type == 'xavier':\n",
    "                init.xavier_normal_(m.weight.data, gain=init_gain)\n",
    "            elif init_type == 'kaiming':\n",
    "                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "            elif init_type == 'orthogonal':\n",
    "                init.orthogonal_(m.weight.data, gain=init_gain)\n",
    "            else:\n",
    "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                init.constant_(m.bias.data, 0.0)\n",
    "        elif classname.find('BatchNorm2d') != -1:    # BatchNorm Layer's weight is not a matrix; only normal distribution applies.\n",
    "            init.normal_(m.weight.data, 1.0, init_gain)\n",
    "            init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    print('initialize network with %s' % init_type)\n",
    "    net.apply(init_func)    # apply the initialization function <init_func>\n",
    "\n",
    "def init_net(net, init_type='normal', init_gain=0.02, gpu_ids=[]):\n",
    "    \"\"\"Initialize a network: 1. register CPU/GPU device (with multi-GPU support); 2. initialize the network weights\n",
    "    Parameters:\n",
    "                    net (network) -- the network to be initialized\n",
    "                    init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n",
    "                    gain (float) -- scaling factor for normal, xavier and orthogonal.\n",
    "                    gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2\n",
    "    Return an initialized network.\n",
    "    \"\"\"\n",
    "    if len(gpu_ids) > 0:\n",
    "        assert(torch.cuda.is_available())\n",
    "        net.to(gpu_ids[0])\n",
    "        net = torch.nn.DataParallel(net, gpu_ids)        # multi-GPUs\n",
    "    init_weights(net, init_type, init_gain=init_gain)\n",
    "    \n",
    "    # Zero for deform convs\n",
    "    key_name_list = ['offset', 'modulator']\n",
    "    for cur_name, parameters in net.named_parameters():\n",
    "        if any(key_name in cur_name for key_name in key_name_list):\n",
    "            nn.init.constant_(parameters, 0.)\n",
    "    return net\n",
    "\n",
    "class ResNetModified(nn.Module):\n",
    "    \"\"\"\n",
    "    Resnet-based generator that consists of deformable Resnet blocks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            input_nc, \n",
    "            output_nc, \n",
    "            ngf=64, \n",
    "            norm_layer=nn.BatchNorm2d, \n",
    "            activation_func=torch.nn.LeakyReLU(negative_slope=0.10, inplace=True),\n",
    "            use_dropout=False, \n",
    "            n_blocks=6, \n",
    "            padding_type='reflect',\n",
    "            upsample_mode='bilinear'):\n",
    "        \"\"\"Construct a Resnet-based generator\n",
    "        Parameters:\n",
    "            input_nc (int) -- the number of channels in input images\n",
    "            output_nc (int) -- the number of channels in output images\n",
    "            ngf (int) -- the number of filters in the last conv layer\n",
    "            norm_layer -- normalization layer\n",
    "            use_dropout (bool) -- if use dropout layers\n",
    "            n_blocks (int) -- the number of ResNet blocks\n",
    "            padding_type (str) -- the name of padding layer in conv layers: reflect | replicate | zero\n",
    "            upsample_mode (str) -- mode for upsampling: transpose | bilinear\n",
    "        \"\"\"\n",
    "        assert(n_blocks >= 0)\n",
    "        super(ResNetModified, self).__init__()\n",
    "        if type(norm_layer) == functools.partial:\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "\n",
    "        # Initial Convolution\n",
    "        self.initial_conv = nn.Sequential(\n",
    "                Conv2d(\n",
    "                        in_channels=input_nc,\n",
    "                        out_channels=ngf,\n",
    "                        kernel_size=7,\n",
    "                        padding_type=padding_type,\n",
    "                        norm_layer=norm_layer,\n",
    "                        activation_func=activation_func,\n",
    "                        use_bias=use_bias),\n",
    "                Conv2d(\n",
    "                        in_channels=ngf,\n",
    "                        out_channels=ngf,\n",
    "                        kernel_size=3,\n",
    "                        padding_type=padding_type,\n",
    "                        norm_layer=norm_layer,\n",
    "                        activation_func=activation_func,\n",
    "                        use_bias=use_bias))\n",
    "\n",
    "        # Downsample Blocks\n",
    "        n_downsampling = 2\n",
    "        mult = 2 ** 0\n",
    "        self.downsample_1 = Conv2d(\n",
    "                in_channels=ngf * mult,\n",
    "                out_channels=ngf * mult * 2,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding_type=padding_type,\n",
    "                norm_layer=norm_layer,\n",
    "                activation_func=activation_func,\n",
    "                use_bias=use_bias)\n",
    "        \n",
    "        mult = 2 ** 1\n",
    "        self.downsample_2 = Conv2d(\n",
    "                in_channels=ngf * mult,\n",
    "                out_channels=ngf * mult * 2,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding_type=padding_type,\n",
    "                norm_layer=norm_layer,\n",
    "                activation_func=activation_func,\n",
    "                use_bias=use_bias)\n",
    "\n",
    "        # Residual Blocks\n",
    "        residual_blocks = []\n",
    "        mult = 2 ** n_downsampling\n",
    "        for i in range(n_blocks): # add ResNet blocks\n",
    "            residual_blocks += [\n",
    "                    DeformableResnetBlock(\n",
    "                            ngf * mult, \n",
    "                            padding_type=padding_type, \n",
    "                            norm_layer=norm_layer, \n",
    "                            use_dropout=use_dropout, \n",
    "                            use_bias=use_bias, activation_func=activation_func)]\n",
    "        \n",
    "        self.residual_blocks = nn.Sequential(*residual_blocks)\n",
    "\n",
    "        # Upsampling\n",
    "        mult = 2 ** (n_downsampling - 0)\n",
    "        self.upsample_2 = DecoderBlock(\n",
    "                ngf * mult, \n",
    "                int(ngf * mult / 2),\n",
    "                int(ngf * mult / 2),\n",
    "                use_bias=use_bias,\n",
    "                activation_func=activation_func,\n",
    "                norm_layer=norm_layer,\n",
    "                padding_type=padding_type,\n",
    "                upsample_mode=upsample_mode)\n",
    "        \n",
    "        mult = 2 ** (n_downsampling - 1)\n",
    "        self.upsample_1 = DecoderBlock(\n",
    "                ngf * mult, \n",
    "                int(ngf * mult / 2),\n",
    "                int(ngf * mult / 2),\n",
    "                use_bias=use_bias,\n",
    "                activation_func=activation_func,\n",
    "                norm_layer=norm_layer,\n",
    "                padding_type=padding_type,\n",
    "                upsample_mode=upsample_mode)\n",
    "        \n",
    "        # Output Convolution\n",
    "        self.output_conv_naive = nn.Sequential(\n",
    "                nn.ReflectionPad2d(1),\n",
    "                nn.Conv2d(ngf, output_nc, kernel_size=3, padding=0),\n",
    "                nn.Tanh())\n",
    "\n",
    "        # # Projection for rain robust loss\n",
    "        # self.feature_projection = nn.Sequential(\n",
    "        #         nn.AdaptiveAvgPool2d((2, 2)),\n",
    "        #         nn.Flatten(start_dim=1, end_dim=-1))\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Standard forward\"\"\"\n",
    "\n",
    "        # Downsample\n",
    "        initial_conv_out    = self.initial_conv(input)\n",
    "        downsample_1_out = self.downsample_1(initial_conv_out)\n",
    "        downsample_2_out = self.downsample_2(downsample_1_out)\n",
    "\n",
    "        # Residual\n",
    "        residual_blocks_out = self.residual_blocks(downsample_2_out)\n",
    "\n",
    "        # Upsample\n",
    "        upsample_2_out = self.upsample_2(residual_blocks_out, downsample_1_out)\n",
    "        upsample_1_out = self.upsample_1(upsample_2_out, initial_conv_out)\n",
    "        final_out = self.output_conv_naive(upsample_1_out)\n",
    "\n",
    "        # Features\n",
    "        # features = self.feature_projection(residual_blocks_out)\n",
    "\n",
    "        # Return multiple final conv results\n",
    "        return final_out, # features\n",
    "\n",
    "class GTRainModel(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            ngf=64,\n",
    "            n_blocks=9,\n",
    "            norm_layer_type='batch',\n",
    "            activation_func=torch.nn.LeakyReLU(negative_slope=0.10, inplace=True),\n",
    "            upsample_mode='bilinear',\n",
    "            init_type='kaiming'):\n",
    "        \"\"\"\n",
    "        GT-Rain Model\n",
    "        Parameters:\n",
    "            ngf (int) -- the number of conv filters\n",
    "            n_blocks (int) -- the number of deformable ResNet blocks\n",
    "            norm_layer_type (str) -- 'batch', 'instance'\n",
    "            activation_func (func) -- activation functions\n",
    "            upsample_mode (str) -- 'transpose', 'bilinear'\n",
    "            init_type (str) -- None, 'normal', 'xavier', 'kaiming', 'orthogonal'\n",
    "        \"\"\"\n",
    "        super(GTRainModel, self).__init__()\n",
    "        self.resnet = ResNetModified(\n",
    "            input_nc=3, output_nc=3, ngf=ngf, \n",
    "            norm_layer=get_norm_layer(norm_layer_type),\n",
    "            activation_func=activation_func,\n",
    "            use_dropout=False, n_blocks=n_blocks, \n",
    "            padding_type='reflect',\n",
    "            upsample_mode=upsample_mode)\n",
    "\n",
    "        # Initialization\n",
    "        if init_type:\n",
    "            init_net(self.resnet, init_type=init_type)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_img = self.resnet(x)\n",
    "        return out_img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23940a42",
   "metadata": {
    "id": "23940a42",
    "outputId": "9a0213d4-59ef-4937-b5dd-d1206e9c4222"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GTRainModel(\n",
       "  (resnet): ResNetModified(\n",
       "    (initial_conv): Sequential(\n",
       "      (0): Conv2d(\n",
       "        (activation_func): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv): Sequential(\n",
       "          (0): ReflectionPad2d((3, 3, 3, 3))\n",
       "          (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Conv2d(\n",
       "        (activation_func): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (downsample_1): Conv2d(\n",
       "      (activation_func): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "      (conv): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (downsample_2): Conv2d(\n",
       "      (activation_func): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "      (conv): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (residual_blocks): Sequential(\n",
       "      (0): DeformableResnetBlock(\n",
       "        (conv_block): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): DeformableConv2d(\n",
       "            (offset_conv): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (modulator_conv): Conv2d(256, 9, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (regular_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "          (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (5): DeformableConv2d(\n",
       "            (offset_conv): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (modulator_conv): Conv2d(256, 9, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (regular_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): DeformableResnetBlock(\n",
       "        (conv_block): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): DeformableConv2d(\n",
       "            (offset_conv): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (modulator_conv): Conv2d(256, 9, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (regular_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "          (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (5): DeformableConv2d(\n",
       "            (offset_conv): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (modulator_conv): Conv2d(256, 9, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (regular_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (2): DeformableResnetBlock(\n",
       "        (conv_block): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): DeformableConv2d(\n",
       "            (offset_conv): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (modulator_conv): Conv2d(256, 9, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (regular_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "          (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (5): DeformableConv2d(\n",
       "            (offset_conv): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (modulator_conv): Conv2d(256, 9, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (regular_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (3): DeformableResnetBlock(\n",
       "        (conv_block): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): DeformableConv2d(\n",
       "            (offset_conv): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (modulator_conv): Conv2d(256, 9, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (regular_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "          (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (5): DeformableConv2d(\n",
       "            (offset_conv): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (modulator_conv): Conv2d(256, 9, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (regular_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): DeformableResnetBlock(\n",
       "        (conv_block): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): DeformableConv2d(\n",
       "            (offset_conv): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (modulator_conv): Conv2d(256, 9, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (regular_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "          (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (5): DeformableConv2d(\n",
       "            (offset_conv): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (modulator_conv): Conv2d(256, 9, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (regular_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (5): DeformableResnetBlock(\n",
       "        (conv_block): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): DeformableConv2d(\n",
       "            (offset_conv): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (modulator_conv): Conv2d(256, 9, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (regular_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "          (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (5): DeformableConv2d(\n",
       "            (offset_conv): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (modulator_conv): Conv2d(256, 9, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (regular_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (6): DeformableResnetBlock(\n",
       "        (conv_block): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): DeformableConv2d(\n",
       "            (offset_conv): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (modulator_conv): Conv2d(256, 9, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (regular_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "          (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (5): DeformableConv2d(\n",
       "            (offset_conv): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (modulator_conv): Conv2d(256, 9, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (regular_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (7): DeformableResnetBlock(\n",
       "        (conv_block): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): DeformableConv2d(\n",
       "            (offset_conv): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (modulator_conv): Conv2d(256, 9, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (regular_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "          (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (5): DeformableConv2d(\n",
       "            (offset_conv): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (modulator_conv): Conv2d(256, 9, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (regular_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (8): DeformableResnetBlock(\n",
       "        (conv_block): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): DeformableConv2d(\n",
       "            (offset_conv): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (modulator_conv): Conv2d(256, 9, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (regular_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "          (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (5): DeformableConv2d(\n",
       "            (offset_conv): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (modulator_conv): Conv2d(256, 9, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (regular_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (upsample_2): DecoderBlock(\n",
       "      (deconv): UpConv2d(\n",
       "        (conv): Conv2d(\n",
       "          (activation_func): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "          (conv): Sequential(\n",
       "            (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "            (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv): Conv2d(\n",
       "        (activation_func): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (upsample_1): DecoderBlock(\n",
       "      (deconv): UpConv2d(\n",
       "        (conv): Conv2d(\n",
       "          (activation_func): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "          (conv): Sequential(\n",
       "            (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "            (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv): Conv2d(\n",
       "        (activation_func): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (output_conv_naive): Sequential(\n",
       "      (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (2): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set model parameters\n",
    "model_params = {\n",
    "  'load_dir': 'path/to/model/ckpt', # Dir to load model weights\n",
    "  'init_type': None, # Initialization type \n",
    "  'norm_layer_type': 'batch', # Normalization type\n",
    "  'activation_func': torch.nn.LeakyReLU(negative_slope=0.10, inplace=True), # Activation function\n",
    "  'upsample_mode': 'bilinear', # Mode for upsampling\n",
    "  'ngf': 64,\n",
    "  'n_blocks': 9}\n",
    "\n",
    "# Make the model\n",
    "model = GTRainModel(\n",
    "  ngf=model_params['ngf'],\n",
    "  n_blocks=model_params['n_blocks'],\n",
    "  norm_layer_type=model_params['norm_layer_type'],\n",
    "  activation_func=model_params['activation_func'],\n",
    "  upsample_mode=model_params['upsample_mode'],\n",
    "  init_type=model_params['init_type'])\n",
    "\n",
    "# Load model checkpoint\n",
    "checkpoint = torch.load(model_params['load_dir']) #, map_location=torch.device('cpu')\n",
    "model.load_state_dict(checkpoint['state_dict'], strict=True)\n",
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd87ea5e",
   "metadata": {
    "id": "fd87ea5e"
   },
   "outputs": [],
   "source": [
    "# Set parameters for the pipeline\n",
    "\n",
    "pipeline_params = {\n",
    "    \"min_size\": 256, # minimum size of height and width of image\n",
    "    \"optical_flow_motion_threshold\": 5, # magnitude threshold for optical flow\n",
    "    \"psnr_illumination_high_dif_threshold\": .5, # difference in input/gt vs output/gt psnr for high threshold \n",
    "    \"psnr_illumination_low_dif_threshold\": 2, #  difference in input/gt vs output/gt psnr for low threshold\n",
    "    \"psnr_illumination_high_threshold\": 25, # high psnr threshold for illumination check\n",
    "    \"psnr_illumination_low_threshold\": 20, # low psnr threshold for illumination check\n",
    "    \"psnr_movement_threshold\": 20, # hard threshold on psnr of input\n",
    "    \"input_frame_num\": 10, # number of input frames to check\n",
    "    \"overlap_percentage\": .3, # the amount of overlap to throw away potential crops\n",
    "    \"optical_flow_window_size\": 25, # window size for optical flow\n",
    "    \"mean_img_num\": 15, # number of images to average for input optical flow (eliminates rain streak/snow flake)\n",
    "    \"mean_frame_skip\": 2, # number of frames to skip when generating mean images (accelerates OF time)\n",
    "    \"gt_frame_skip\": 3, # number of frames to skip when generating the gt of (accelrates OF time)\n",
    "    \"downsample_of\": True, # whether or not to downsample the optical flow (accelerates OF time)\n",
    "    \"chromatic_var_threshold\": .2, # threshold for chromatic variation property\n",
    "    \"sobel_blur_threshold\": -.05, # threshold for bluriness check with sobel filter\n",
    "    \"sobel_ksize\": 15, # kernel size for sobel filter bluriness check\n",
    "    \"sobel_scale\": 1, # scale for sobel filter bluriness check\n",
    "    \"gaussian_ksize\": 7, # kernel size for gaussian blur before sobel filtering\n",
    "    \"fft_blur_threshold\": -.05, # threshold for bluriness check with fft filter\n",
    "    \"fft_cutoff\": 30, # cutoff for the fft in pixels\n",
    "  }\n",
    "\n",
    "pipeline_params[\"se_movement_threshold\"] = (10**(-.1*pipeline_params[\"psnr_movement_threshold\"]))**.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9826a8",
   "metadata": {
    "id": "3f9826a8"
   },
   "outputs": [],
   "source": [
    "def get_optical_flows(video_path, input_frame_num, mean_img_num, mean_frame_skip):\n",
    "    \n",
    "    \"\"\"\n",
    "    Method for getting optical flow filter masks, also returns the degraded images and checks for corrupted video\n",
    "    Parameters:\n",
    "        video_path: path to input video with weather effects\n",
    "        input_frame_num: number of input frames from video to take into account for pipeline\n",
    "        mean_img_num: number of input frames to take the mean over to eliminate rain streaks/snow flakes\n",
    "        mean_frame_skip: number of input frames to skip over when taking the OF of the averaged images\n",
    "    Output:\n",
    "        binary_mask: filter mask detecting pixels that are moving from optical flow, shape:(H, W, 1)\n",
    "        degraded_imgs: stored frames for degraded weather effects, shape: (input_frame_num, H, W, 3)\n",
    "        is_video: boolean determining whether or not video can be read (may be corrupted)\n",
    "    \"\"\"\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    cap_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    img_queue = []\n",
    "    degraded_imgs = []\n",
    "    is_video = False\n",
    "    \n",
    "    i = 0\n",
    "    first_run = True\n",
    "    frame_skip = cap_frames//input_frame_num\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB).astype(np.float32)/255.0\n",
    "        if i % frame_skip == 0:\n",
    "            degraded_imgs.append(frame)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        if (pipeline_params[\"downsample_of\"]):\n",
    "            h, w = frame.shape\n",
    "            frame = cv2.resize(frame, (w//2, h//2))\n",
    "        img_queue.append(frame)\n",
    "        if i >= mean_img_num-1:\n",
    "            if i % mean_frame_skip == 0:\n",
    "                stacked_imgs = np.stack(img_queue)\n",
    "                pixel_wise_means = np.mean(stacked_imgs, axis=0)\n",
    "                if first_run:\n",
    "                    first_run = False\n",
    "                    is_video = True\n",
    "                    binary_mask = np.zeros(frame.shape, dtype=np.dtype(bool))\n",
    "                else:\n",
    "                    flow = cv2.calcOpticalFlowFarneback(pixel_wise_means, prev_mean, None, 0.5, 3, pipeline_params[\"optical_flow_window_size\"], 3, 5, 1.2, 0)\n",
    "                    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "                    gt_of_mask = mag>pipeline_params[\"optical_flow_motion_threshold\"]\n",
    "                    binary_mask = np.bitwise_or(binary_mask, gt_of_mask)\n",
    "                prev_mean = pixel_wise_means\n",
    "            img_queue.pop(0)\n",
    "        i += 1\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    if not is_video:\n",
    "        return None, None, is_video\n",
    "    \n",
    "    if pipeline_params[\"downsample_of\"]:\n",
    "        h, w = binary_mask.shape\n",
    "        binary_mask = cv2.resize(binary_mask.astype(np.uint8), (w*2, h*2)).astype(bool)\n",
    "    return binary_mask, degraded_imgs, is_video\n",
    "\n",
    "def get_optical_flow_clean(video_path, gt_frame_skip):\n",
    "    \n",
    "    \"\"\"\n",
    "    Grabbing the optical flow filter mask of gt videos, also grabs the gt frame and a boolean for corrupted videos\n",
    "    Parameters:\n",
    "        video_path: path to gt video with no weather effects\n",
    "        gt_frame_skip: number of frames to skip when taking the gt OF\n",
    "    Output:\n",
    "        gt_of_mask: binary filter mask denoting pixel movement in gt frame, shape: (H, W, 1)\n",
    "        gt_img: gt frame with no weather effects, shape: (H, W, 3)\n",
    "        is_video: boolean determining whether or not video can be read (may be corrupted)\n",
    "    \"\"\"\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    cap_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    is_video = False\n",
    "    \n",
    "    i = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if i == 0:\n",
    "            gt_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB).astype(np.float32)/255.0\n",
    "            prev = cv2.cvtColor(gt_img, cv2.COLOR_RGB2GRAY)\n",
    "            is_video = True\n",
    "        if i == gt_frame_skip:\n",
    "            cur = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY).astype(np.float32)/255.0\n",
    "            flow = cv2.calcOpticalFlowFarneback(cur, prev, None, 0.5, 3, pipeline_params[\"optical_flow_window_size\"], 3, 5, 1.2, 0)\n",
    "            mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "            gt_of_mask = mag>pipeline_params[\"optical_flow_motion_threshold\"]\n",
    "            break\n",
    "        i += 1\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    if not is_video:\n",
    "        return None, None, is_video\n",
    "    \n",
    "    return gt_of_mask, gt_img, is_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3c9d99",
   "metadata": {
    "id": "3c3c9d99"
   },
   "outputs": [],
   "source": [
    "def cal_skyline(mask):\n",
    "    \"\"\"\n",
    "    Helper function for sky segmentation\n",
    "    Median filter and threshold of gradient for sky segmentation\n",
    "    \"\"\"\n",
    "    h, w = mask.shape\n",
    "    for i in range(w):\n",
    "        raw = mask[:, i]\n",
    "        after_median = medfilt(raw, 19)\n",
    "        try:\n",
    "            first_zero_index = np.where(after_median == 0)[0][0]\n",
    "            first_one_index = np.where(after_median == 1)[0][0]\n",
    "            if first_zero_index > 20:\n",
    "                mask[first_one_index:first_zero_index, i] = 1\n",
    "                mask[first_zero_index:, i] = 0\n",
    "                mask[:first_one_index, i] = 0\n",
    "        except:\n",
    "            continue\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_sky_region_gradient(img):\n",
    "    \n",
    "    \"\"\"\n",
    "    Uses image gradients to segment the sky region out\n",
    "    Output:\n",
    "        a filter mask removing the sky portion of an image\n",
    "    \"\"\"\n",
    "    \n",
    "    img = (img*255).astype(np.uint8)\n",
    "\n",
    "    h, w, _ = img.shape\n",
    "\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    img_gray = cv2.blur(img_gray, (9, 3))\n",
    "    cv2.medianBlur(img_gray, 5)\n",
    "    lap = cv2.Laplacian(img_gray, cv2.CV_8U)\n",
    "    gradient_mask = (lap < 6).astype(np.uint8)\n",
    "\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (9, 3))\n",
    "    mask = cv2.morphologyEx(gradient_mask, cv2.MORPH_ERODE, kernel)\n",
    "    mask = cal_skyline(mask)\n",
    "\n",
    "    kernel_2 = np.ones((5,5), dtype=np.uint8)\n",
    "    mask_refined = cv2.erode(mask, kernel_2, iterations=5)\n",
    "\n",
    "    return mask_refined>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcda213e",
   "metadata": {
    "id": "bcda213e"
   },
   "outputs": [],
   "source": [
    "def restore_degraded(degraded_imgs):\n",
    "    \"\"\"\n",
    "    Restores degraded images using seed model\n",
    "    Parameters:\n",
    "        degraded_imgs: degraded images in shape: (input_frame_num, H, W, 3)\n",
    "    Output:\n",
    "        restored_imgs: restored images in shape: (input_frame_num, H, W, 3)\n",
    "    \"\"\"\n",
    "    restored_imgs = []\n",
    "    for degraded_img in degraded_imgs:\n",
    "        model_input = torch.from_numpy(degraded_img).permute((2, 0, 1)) * 2 - 1\n",
    "        model_input = torch.unsqueeze(model_input, 0).cuda()\n",
    "        with torch.no_grad():\n",
    "            output = (model(model_input)[0]* 0.5 + 0.5).squeeze().permute((1, 2, 0))\n",
    "        restored_img = output.detach().cpu().numpy()\n",
    "        restored_imgs.append(restored_img)\n",
    "    model_input = None\n",
    "    output = None\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return restored_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea14e25a",
   "metadata": {
    "id": "ea14e25a"
   },
   "outputs": [],
   "source": [
    "def generate_static_obj_masks(restored_imgs, gt_img):\n",
    "    \"\"\"\n",
    "    Generates binary filter mask for detecting static object discrepancies between input and gt frames\n",
    "    Parameter:\n",
    "        restored_imgs: restored images, output from seed model, shape: (input_frame_num, H, W, 3)\n",
    "        gt_img: gt frame, shape: (H, W, 3)\n",
    "    Output:\n",
    "        mask: binary filter mask determing which pixels have the presence of a static object discrepancy\n",
    "    \"\"\"\n",
    "    mask = np.zeros((gt_img.shape[0], gt_img.shape[1]), dtype=np.dtype(bool)) \n",
    "    for idx in range(len(restored_imgs)):\n",
    "        restored_img = restored_imgs[idx]\n",
    "        frame_mask = (np.mean(np.abs(restored_img - gt_img), axis=2)>pipeline_params[\"se_movement_threshold\"])\n",
    "        mask = np.bitwise_or(mask, frame_mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8c73df",
   "metadata": {
    "id": "0f8c73df"
   },
   "outputs": [],
   "source": [
    "def chromatic_variation(degraded_img, gt_img):\n",
    "    \"\"\"\n",
    "    Gets the binary mask denoting pixels that do not follow the chromatic variation constraint\n",
    "        Reference Section \"Filtering Block Two: Color Verification\" in Section 4\n",
    "    Parameters:\n",
    "        degraded_img: a single degraded frame with weather effects\n",
    "        gt_img: a gt frame without weather effects\n",
    "    Output:\n",
    "        binary mask denoting pixels that do not follow the chromatic variation constraint\n",
    "            used to detect static object variations\n",
    "    \"\"\"\n",
    "    diff_map_max = np.max(degraded_img-gt_img, axis=2)\n",
    "    diff_map_min = np.min(degraded_img-gt_img, axis=2)\n",
    "    diff_map = (diff_map_max - diff_map_min)\n",
    "    threshold_map = diff_map > pipeline_params[\"chromatic_var_threshold\"]\n",
    "    kernel = np.ones((21,21), np.uint8)\n",
    "    threshold_map_closed = cv2.morphologyEx(threshold_map.astype(np.uint8), cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "    return threshold_map_closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe5a29d",
   "metadata": {
    "id": "abe5a29d"
   },
   "outputs": [],
   "source": [
    "def overlap_percent(XA1, YA1, XA2, YA2, XB1, YB1, XB2, YB2, SA, SB):\n",
    "    \"\"\"\n",
    "    Helper function for cropping function\n",
    "    Calculates the IOU of two crops\n",
    "    Parameters:\n",
    "        XA1, YA1, XA2, YA2: coordinates of first rectangle: left x, top y, right x, bottom y\n",
    "        XB1, YB1, XB2, YB2: coordinates of second rectangle: left x, top y, right x, bottom y\n",
    "    \"\"\"\n",
    "    SI = max(0, min(XA2, XB2) - max(XA1, XB1)) * max(0, min(YA2, YB2) - max(YA1, YB1))\n",
    "    SU = SA + SB - SI\n",
    "    return SI/SU\n",
    "\n",
    "\n",
    "def allRectangle(matrix):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates all possible rectangular crops bigger than \"min_size\", keeps overlap below \"overlap_percentage\"\n",
    "    Parameters:\n",
    "        matrix: binary validity map for crops\n",
    "    Output:\n",
    "        crop_dict: output crop dictionary in the form: crop_dict[(y, x)] = (height, width)\n",
    "    \"\"\"\n",
    "    n = matrix.shape[1] \n",
    "    height = [0] * (n + 1)\n",
    "    ans = 0\n",
    "    crop_dict = {}\n",
    "    for row_idx in range(matrix.shape[0]):\n",
    "        for i in range(n):\n",
    "            height[i] = height[i] + 1 if matrix[row_idx][i] == 1 else 0\n",
    "        stack = [-1]\n",
    "        for i in range(n + 1):\n",
    "            while height[i] < height[stack[-1]]:\n",
    "                h = height[stack.pop()]\n",
    "                w = i - 1 - stack[-1]\n",
    "                if h >= pipeline_params[\"min_size\"] and w >= pipeline_params[\"min_size\"]:\n",
    "                    rect_row = row_idx -h + 1\n",
    "                    rect_col = i\n",
    "                    if (rect_row, rect_col) in crop_dict:\n",
    "                        dict_val = crop_dict[(rect_row, rect_col)]\n",
    "                        if h*w > dict_val[0]*dict_val[1]:\n",
    "                            crop_dict[(rect_row, rect_col)] = (h, w)\n",
    "                    else:\n",
    "                        overlap_bool = False\n",
    "                        for (row2, col2) in crop_dict:\n",
    "                            (h2, w2) = crop_dict[(row2, col2)]\n",
    "                            overlap = overlap_percent(rect_col-w, rect_row, rect_col, rect_row+h,\n",
    "                                                     col2-w2, row2, col2, row2+h2, h*w, h2*w2)\n",
    "                            if overlap > pipeline_params[\"overlap_percentage\"]:\n",
    "                                if h*w > h2*w2:\n",
    "                                    crop_dict.pop((row2,col2))\n",
    "                                else:\n",
    "                                    overlap_bool = True\n",
    "                                break\n",
    "                        if not overlap_bool:\n",
    "                            crop_dict[(rect_row, rect_col)] = (h, w)\n",
    "            stack.append(i)\n",
    "    return crop_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f516dbc",
   "metadata": {
    "id": "8f516dbc"
   },
   "outputs": [],
   "source": [
    "def psnr_illumination_check(degraded_imgs, restored_imgs, gt_img, crop_dict):\n",
    "    \"\"\"\n",
    "    Check for illumination shift using hysteresis of psnr of input/gt vs. psnr of restored/gt using seed model\n",
    "    Parameters:\n",
    "        degraded_imgs: input images of shape: (input_frame_num, H, W, 3)\n",
    "        restored_imgs: restored images from seed model of shape: (input_frame_num, H, W, 3)\n",
    "        gt_img: gt image of shape: (H, W, 3)\n",
    "        crop_dict: crop dictionary in the form: crop_dict[(y, x)] = (height, width)\n",
    "    Output:\n",
    "        accepted_list: list of all accepted scenes, \n",
    "            element is tuple of form (degraded/gt psnr, restored/gt psnr, y, x, h, w)\n",
    "            stores psnr values and crop locations\n",
    "    \"\"\"\n",
    "    accepted_list = []\n",
    "    crop_psnr_in_dict = {}\n",
    "    crop_psnr_out_dict = {}\n",
    "    for idx in range(len(degraded_imgs)):\n",
    "        degraded_img = degraded_imgs[idx]\n",
    "        restored_img = restored_imgs[idx]\n",
    "\n",
    "        for (row, col) in crop_dict:\n",
    "            (h, w) = crop_dict[(row, col)]\n",
    "            gt_crop = gt_img[row:row+h, col-w:col, :]\n",
    "            degraded_crop = degraded_img[row:row+h, col-w:col, :]\n",
    "            restored_crop = restored_img[row:row+h, col-w:col, :]\n",
    "            psnr_in = psnr(degraded_crop, gt_crop)\n",
    "            psnr_out = psnr(restored_crop, gt_crop)\n",
    "            if (row, col) in crop_psnr_in_dict:\n",
    "                crop_psnr_in_dict[(row, col)] += psnr_in\n",
    "                crop_psnr_out_dict[(row, col)] += psnr_out\n",
    "            else:\n",
    "                crop_psnr_in_dict[(row, col)] = psnr_in\n",
    "                crop_psnr_out_dict[(row, col)] = psnr_out\n",
    "    for (row, col) in crop_dict:\n",
    "        (h, w) = crop_dict[(row, col)]\n",
    "        psnr_in = crop_psnr_in_dict[(row, col)]/len(degraded_imgs)\n",
    "        psnr_out = crop_psnr_out_dict[(row, col)]/len(degraded_imgs)\n",
    "        if (psnr_out > pipeline_params[\"psnr_illumination_high_threshold\"]\n",
    "           and psnr_out-psnr_in > pipeline_params[\"psnr_illumination_high_dif_threshold\"]):\n",
    "            accepted_list.append((psnr_in, psnr_out, row, col-w, h, w))\n",
    "            continue\n",
    "        if (psnr_out > pipeline_params[\"psnr_illumination_low_threshold\"]\n",
    "           and psnr_out-psnr_in > pipeline_params[\"psnr_illumination_low_dif_threshold\"]):\n",
    "            accepted_list.append((psnr_in, psnr_out, row, col-w, h, w))\n",
    "    return accepted_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2880bb1",
   "metadata": {
    "id": "e2880bb1"
   },
   "outputs": [],
   "source": [
    "def calcMetricSobel(degraded, gt, ksize, scale, gaussian_ksize):\n",
    "    \"\"\"\n",
    "    Calculates a metric for bluriness based on the Sobel gradient filter\n",
    "    Parameters:\n",
    "        degraded: degraded image\n",
    "        gt: gt image\n",
    "        ksize: kernel size of Sobel filter\n",
    "        scale: scale of Sobel filter\n",
    "        gaussian_ksize: kernel size of Gaussian blur filter before Sobel filter\n",
    "    Output:\n",
    "        Gives metric based on magnitude of gradients, normalized\n",
    "    \"\"\"\n",
    "    gt = cv2.cvtColor(gt, cv2.COLOR_BGR2GRAY)\n",
    "    degraded = cv2.cvtColor(degraded, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    gt_blur = cv2.GaussianBlur(gt, (gaussian_ksize, gaussian_ksize), sigmaX=0)\n",
    "    gt_var_x = np.square(cv2.Sobel(gt_blur, cv2.CV_64F, 1, 0, ksize=ksize, scale=scale))\n",
    "    gt_var_y = np.square(cv2.Sobel(gt_blur, cv2.CV_64F, 0, 1, ksize=ksize, scale=scale))\n",
    "    gt_var = np.sqrt(gt_var_x + gt_var_y).mean()\n",
    "    degraded_blur = cv2.GaussianBlur(degraded, (gaussian_ksize, gaussian_ksize), sigmaX=0)\n",
    "    degraded_var_x = np.square(cv2.Sobel(degraded_blur, cv2.CV_64F, 1, 0, ksize=ksize, scale=scale))\n",
    "    degraded_var_y = np.square(cv2.Sobel(degraded_blur, cv2.CV_64F, 0, 1, ksize=ksize, scale=scale))\n",
    "    degraded_var = np.sqrt(degraded_var_x + degraded_var_y).mean()\n",
    "    \n",
    "    return (gt_var - degraded_var)/gt_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8689e91",
   "metadata": {
    "id": "b8689e91"
   },
   "outputs": [],
   "source": [
    "# FFT Metric from https://pyimagesearch.com/2020/06/15/opencv-fast-fourier-transform-fft-for-blur-detection-in-images-and-video-streams/\n",
    "def calcMetricFFT(degraded, gt, cutoff):\n",
    "    \"\"\"\n",
    "    Calculates a metric for bluriness based on the FFT\n",
    "    Parameters:\n",
    "        degraded: degraded image\n",
    "        gt: gt image\n",
    "        cutoff: cutoff to use for low-pass filter\n",
    "    Output:\n",
    "        Gives metric based on output magnitude after low-pass filtering, normalized\n",
    "    \"\"\"\n",
    "    gt = cv2.cvtColor(gt, cv2.COLOR_RGB2GRAY)\n",
    "    degraded = cv2.cvtColor(degraded, cv2.COLOR_RGB2GRAY)\n",
    "    (h, w) = degraded.shape\n",
    "    (cX, cY) = (int(w / 2.0), int(h / 2.0))\n",
    "    \n",
    "    gt_fft = np.fft.fft2(gt)\n",
    "    gt_fftShift = np.fft.fftshift(gt_fft)\n",
    "    gt_fftShift[cY - cutoff:cY + cutoff, cX - cutoff:cX + cutoff] = 0\n",
    "    gt_fftShift = np.fft.ifftshift(gt_fftShift)\n",
    "    gt_recon = np.fft.ifft2(gt_fftShift)\n",
    "    gt_mag = 20*np.log(np.abs(gt_recon)).mean()\n",
    "    \n",
    "    degraded_fft = np.fft.fft2(degraded)\n",
    "    degraded_fftShift = np.fft.fftshift(degraded_fft)\n",
    "    degraded_fftShift[cY - cutoff:cY + cutoff, cX - cutoff:cX + cutoff] = 0\n",
    "    degraded_fftShift = np.fft.ifftshift(degraded_fftShift)\n",
    "    degraded_recon = np.fft.ifft2(degraded_fftShift)\n",
    "    degraded_mag = 20*np.log(np.abs(degraded_recon)).mean()\n",
    "    \n",
    "    return (gt_mag-degraded_mag)/gt_mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29342f6",
   "metadata": {
    "id": "f29342f6"
   },
   "outputs": [],
   "source": [
    "def multi_scatter_check(degraded_imgs, gt_img, accepted_list):\n",
    "    \"\"\"\n",
    "    Checks to see if degraded and gt frames conform to the multi-scatter constraint\n",
    "    Reference Section \"Filtering Block Three: Multi-scatter Verification\" in Section 4\n",
    "    Parameters:\n",
    "        degraded_imgs: degraded frames with weather effects\n",
    "        gt_img: gt frames without weather effects\n",
    "        accepted_list: list of accepted crops output from the illumination shift check\n",
    "    Output:\n",
    "        filtered_accepted_list: list of accepted crops with same elements,\n",
    "            but with crops violating multi-scatter constrain removed\n",
    "    \"\"\"\n",
    "    filtered_accepted_list = []\n",
    "    for idx in range(len(accepted_list)):\n",
    "        (psnr_in, psnr_out, row, col, h, w) = accepted_list[i]\n",
    "\n",
    "        degraded_expanded = np.expand_dims((degraded_imgs[0][row:row+h, col:col+w, :]*255).astype(np.uint8), axis=0)\n",
    "        for i in range(1, len(degraded_imgs)):\n",
    "            next_degraded = np.expand_dims((degraded_imgs[i][row:row+h, col:col+w, :]*255).astype(np.uint8), axis=0)\n",
    "            degraded_expanded = np.vstack((degraded_expanded, next_degraded))\n",
    "        degraded_avg = np.mean(degraded_expanded, axis=0).astype(np.uint8)\n",
    "\n",
    "        gt_img_uint8 = (gt_img[row:row+h, col:col+w, :]*255).astype(np.uint8)\n",
    "        metric1 = calcMetricSobel(degraded_avg, gt_img_uint8, \n",
    "                                  pipeline_params[\"sobel_ksize\"],\n",
    "                                  pipeline_params[\"sobel_scale\"],\n",
    "                                  pipeline_params[\"gaussian_ksize\"])\n",
    "        metric2 = calcMetricFFT(degraded_avg, gt_img_uint8, pipeline_params[\"fft_cutoff\"])\n",
    "        \n",
    "        if metric1 > pipeline_params[\"sobel_blur_threshold\"] and metric2 > pipeline_params[\"fft_blur_threshold\"]:\n",
    "            filtered_accepted_list.append((psnr_in, psnr_out, row, col, h, w))\n",
    "    return filtered_accepted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2184588",
   "metadata": {
    "id": "e2184588"
   },
   "outputs": [],
   "source": [
    "def save_scenes(accepted_list, video_in, gt_video, gt_img, out_dir):\n",
    "    \"\"\"\n",
    "    saves frames and json dictionary to an output directory\n",
    "    Parameters:\n",
    "        accepted_list: list of all accepted scenes, \n",
    "            element is tuple of form (degraded/gt psnr, restored/gt psnr, y, x, h, w)\n",
    "        video_in: input video path with weather degradations\n",
    "        gt_video: gt video path with no weather degradations\n",
    "        gt_img: gt frame with no weather degradations\n",
    "        out_dir: output directory with location to save the json and frames\n",
    "    Saves:\n",
    "        logs_dict: json file with information regarding scene:\n",
    "            gt_video: video path where gt frame was taken from\n",
    "            degraded_video: video path where degraded frames were taken from\n",
    "            crop: (y, x, h, w) location for crop\n",
    "            psnr_in: degraded/gt psnr\n",
    "            psnr_out: restored/gt psnr\n",
    "        Frames Directory Structure:\n",
    "            out_dir\n",
    "                gt.png\n",
    "                degraded_0.png\n",
    "                degraded_1.png\n",
    "                    ...\n",
    "                degraded_n.png\n",
    "    \"\"\"\n",
    "    for i in range(len(accepted_list)):\n",
    "        out_dir_path = f\"{out_dir}_{i}/\"\n",
    "        os.makedirs(out_dir_path, exist_ok=True)\n",
    "        \n",
    "        logs_dict = {}\n",
    "        (psnr_in, psnr_out, row, col, h, w) = accepted_list[i]\n",
    "        logs_dict['gt_video'] = gt_video\n",
    "        logs_dict['degraded_video'] = video_in\n",
    "        logs_dict['crop'] = (row, col, h, w)\n",
    "        logs_dict['psnr_in'] = psnr_in\n",
    "        logs_dict['psnr_out'] = psnr_out\n",
    "        with open(f\"{out_dir}_{i}/logs_dict.json\", \"w\") as outfile:\n",
    "            json.dump(logs_dict, outfile)\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_in)\n",
    "    cap_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    idx = 0\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        for i in range(len(accepted_list)):\n",
    "            (psnr_in, psnr_out, row, col, h, w) = accepted_list[i]\n",
    "            out_dir_path = f\"{out_dir}_{i}/\"\n",
    "            Image.fromarray((gt_img[row:row+h, col:col+w, :]*255).astype(np.uint8)).save(f'{out_dir_path}gt.png')\n",
    "            degraded = frame[row:row+h, col:col+w, :]\n",
    "            cv2.imwrite(f'{out_dir_path}degraded_{idx}.png', degraded)\n",
    "        idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4448c2",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "c828d9d3bc5648628a76dae28b7bd94b"
     ]
    },
    "id": "1c4448c2",
    "outputId": "2c031704-a6a4-4e27-d9ef-cdf5a556d7d6",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01835942268371582,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 53,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c828d9d3bc5648628a76dae28b7bd94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206_0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main pipeline loop\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Specify folder name with location of videos\n",
    "Video directory structure should be:\n",
    "folder_name\n",
    "    scene_name\n",
    "        gt\n",
    "            (gt_video_name).mp4\n",
    "        degraded\n",
    "            (degraded_video_name).mp4\n",
    "\"\"\"\n",
    "folder_name = \"path/to/folder_name\"\n",
    "scene_paths = natsorted(glob(f\"{folder_name}/*\"))\n",
    "for scene_path in tqdm(scene_paths[:]):\n",
    "    scene_name = scene_path.split('/')[-1]\n",
    "    print(scene_name)\n",
    "    # Find all degraded and gt video paths\n",
    "    video_files_degraded = natsorted(glob(f\"{scene_path}/degraded/*.mp4\"))\n",
    "    video_files_gt = natsorted(glob(f\"{scene_path}/gt/*.mp4\"))\n",
    "    # Found scene flag\n",
    "    found_scene = False\n",
    "    # Loop through all degraded videos\n",
    "    for degraded_video in video_files_degraded[-1::-3]:\n",
    "        if found_scene:\n",
    "            break\n",
    "        # Get optical flow masks and degraded frames\n",
    "        degraded_video_name = degraded_video.split('/')[-1][:-4]\n",
    "        degraded_of, degraded_imgs, is_video = get_optical_flows(degraded_video, \n",
    "                                                 pipeline_params[\"input_frame_num\"], \n",
    "                                                 pipeline_params[\"mean_img_num\"], \n",
    "                                                 pipeline_params[\"mean_frame_skip\"])\n",
    "        # Continue loop if video is corrupted\n",
    "        if not is_video:\n",
    "            continue\n",
    "\n",
    "        # Get restored images\n",
    "        restored_imgs = restore_degraded(degraded_imgs)\n",
    "        \n",
    "        # Loop through gt videos\n",
    "        for gt_video in video_files_gt[::3]:\n",
    "            if found_scene:\n",
    "                break\n",
    "            \n",
    "            # Get optical flow masks and gt frame\n",
    "            gt_video_name = gt_video.split('/')[-1][:-4]\n",
    "            gt_of, gt_img, is_video = get_optical_flow_clean(gt_video, pipeline_params[\"gt_frame_skip\"])\n",
    "\n",
    "            # Continue loop if video is corrupted or gt frame is different size than rainy frame\n",
    "            if not is_video:\n",
    "                continue\n",
    "            if ((not gt_img.shape[1] == degraded_imgs[0].shape[1]) or (not gt_img.shape[0] == degraded_imgs[0].shape[0])):\n",
    "                continue\n",
    "\n",
    "            # Get static object discrepancy binary mask\n",
    "            static_obj_mask = generate_static_obj_masks(restored_imgs, gt_img)\n",
    "\n",
    "            # Get sky segmentation binary mask\n",
    "            sky_mask = get_sky_region_gradient(gt_img)\n",
    "            \n",
    "            # Get chromatic variation binary mask\n",
    "            chrom_var_mask = chromatic_variation(degraded_imgs[0], gt_img)\n",
    "\n",
    "            # Union of filter masks\n",
    "            of_mask = np.bitwise_or(degraded_of, gt_of)\n",
    "            discrepancy_mask = np.bitwise_or(of_mask, static_obj_mask)\n",
    "            discrepancy_mask_final = np.bitwise_or(discrepancy_mask, chrom_var_mask)\n",
    "            final_mask = np.bitwise_or(discrepancy_mask_final, sky_mask)\n",
    "\n",
    "            # Smooth mask then find all suitable rectangular crops\n",
    "            inv_binary_mask = np.invert(final_mask).astype(np.uint8)\n",
    "            kernel = np.ones((9,9), np.uint8)\n",
    "            inv_binary_mask = cv2.morphologyEx(inv_binary_mask, cv2.MORPH_CLOSE, kernel)\n",
    "            crop_dict = allRectangle(inv_binary_mask)\n",
    "\n",
    "            # If no crops available, continue with next loop iteration\n",
    "            if (len(crop_dict) == 0):\n",
    "                continue\n",
    "\n",
    "            # Find all accepted crops based on illumination shift check\n",
    "            accepted_list = psnr_illumination_check(degraded_imgs, restored_imgs, gt_img, crop_dict)\n",
    "            # Find all accepted crops based on multi scatter check\n",
    "            accepted_list = multi_scatter_check(degraded_imgs, gt_img, accepted_list)\n",
    "\n",
    "            # If no crops accepted, continue with next loop iteration\n",
    "            if (len(accepted_list) == 0):\n",
    "                continue\n",
    "            else:\n",
    "                # If crops accepted, save frames and json dictionary\n",
    "                found_scene = True\n",
    "                save_scenes(accepted_list, degraded_video, gt_video, gt_img, f\"{folder_name}_out/{scene_name}\")\n",
    "                print(\"Found scene!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a19f8d",
   "metadata": {
    "id": "33a19f8d"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
